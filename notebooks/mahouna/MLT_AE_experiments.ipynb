{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a66b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Subset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83eb926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14015ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(size=(64,64)):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    ])\n",
    "\n",
    "class RelabeledDataset(Dataset):\n",
    "    def __init__(self, base, mapping, keep_names=None):\n",
    "        self.base = base\n",
    "        if keep_names is None:\n",
    "            self.indices = list(range(len(base.samples)))\n",
    "        else:\n",
    "            keep = set(keep_names)\n",
    "            self.indices = [i for i,(_,y) in enumerate(base.samples) if base.classes[y] in keep]\n",
    "        self.mapping = mapping\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[self.indices[idx]]\n",
    "        return x, self.mapping[y]\n",
    "\n",
    "def build_binary_dataset(root, keep=(\"Forest\",\"Residential\"), size=(64,64)):\n",
    "    base = datasets.ImageFolder(root, transform=make_transform(size))\n",
    "    mapping = {base.class_to_idx[keep[0]]:0, base.class_to_idx[keep[1]]:1}\n",
    "    ds = RelabeledDataset(base, mapping, keep_names=keep)\n",
    "    return ds, base, mapping\n",
    "\n",
    "def relabel_imagefolder(root, mapping, size=(64,64)):\n",
    "    base = datasets.ImageFolder(root, transform=make_transform(size))\n",
    "    ds = RelabeledDataset(base, mapping, keep_names=None)\n",
    "    return ds, base\n",
    "\n",
    "def make_loaders(dataset, batch_size=32, val_ratio=0.2, seed=42, num_workers=0):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    n = len(dataset)\n",
    "    n_train = int((1 - val_ratio) * n)\n",
    "    n_val = n - n_train\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=g)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=g, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def indices_by_class(base, name):\n",
    "    c = base.class_to_idx[name]\n",
    "    return [i for i,(_,y) in enumerate(base.samples) if y == c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04141e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, logits_fn, device):\n",
    "    model.eval()\n",
    "    tot = 0\n",
    "    ok = 0\n",
    "    cm = torch.zeros(2,2, dtype=torch.int64)\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = logits_fn(model, x)\n",
    "            pred = logits.argmax(1)\n",
    "            ok += (pred == y).sum().item()\n",
    "            tot += y.size(0)\n",
    "            for t,p in zip(y.cpu(), pred.cpu()):\n",
    "                cm[t, p] += 1\n",
    "    acc = ok / tot if tot else float(\"nan\")\n",
    "    return acc, cm\n",
    "\n",
    "def train_classifier(model, train_loader, val_loader, device, lr=1e-3, epochs=30, patience=10):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    best = float(\"inf\")\n",
    "    bad = 0\n",
    "    hist = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        run = 0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            run += loss.item()\n",
    "        run /= len(train_loader)\n",
    "        model.eval()\n",
    "        val = 0.0\n",
    "        ok, tot = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = ce(logits, y)\n",
    "                val += loss.item()\n",
    "                ok += (logits.argmax(1)==y).sum().item()\n",
    "                tot += y.size(0)\n",
    "        val /= len(val_loader)\n",
    "        acc = ok/tot\n",
    "        hist.append((run, val, acc))\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {run:.4f} | Val Loss: {val:.4f} | Val Acc: {acc:.4f}\")\n",
    "        if val < best:\n",
    "            best = val\n",
    "            bad = 0\n",
    "            best_w = model.state_dict()\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    model.load_state_dict(best_w)\n",
    "    return model, hist\n",
    "\n",
    "def train_mtl(model, train_loader, val_loader, device, alpha=0.6, lr=1e-3, epochs=30, patience=10):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    rec = nn.MSELoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    best = float(\"inf\")\n",
    "    bad = 0\n",
    "    hist = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        tr = 0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits, recon = model(x)\n",
    "            lc = ce(logits, y)\n",
    "            lr_ = rec(recon, x)\n",
    "            loss = alpha*lc + (1-alpha)*lr_\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr += loss.item()\n",
    "        tr /= len(train_loader)\n",
    "        model.eval()\n",
    "        vl, ok, tot = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                logits, recon = model(x)\n",
    "                lc = ce(logits, y)\n",
    "                lr_ = rec(recon, x)\n",
    "                loss = alpha*lc + (1-alpha)*lr_\n",
    "                vl += loss.item()\n",
    "                ok += (logits.argmax(1)==y).sum().item()\n",
    "                tot += y.size(0)\n",
    "        vl /= len(val_loader)\n",
    "        acc = ok/tot\n",
    "        hist.append((tr, vl, acc))\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {tr:.4f} | Val Loss: {vl:.4f} | Val Acc: {acc:.4f}\")\n",
    "        if vl < best:\n",
    "            best = vl\n",
    "            bad = 0\n",
    "            best_w = model.state_dict()\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    model.load_state_dict(best_w)\n",
    "    return model, hist\n",
    "\n",
    "def train_soft_share(model, train_loader, val_loader, device, alpha=0.9, beta=0.09, gamma=0.01, mask_ratio=0.65, lr=1e-3, epochs=30, patience=10):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    best = float(\"inf\")\n",
    "    bad = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        tr = 0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            m = (torch.rand_like(x[:, :1]) < mask_ratio).float()\n",
    "            xm = x*(1-m)\n",
    "            opt.zero_grad()\n",
    "            logits, recon, zc, zr = model(xm)\n",
    "            lc = ce(logits, y)\n",
    "            l1 = torch.abs(recon - x)*m\n",
    "            lr_ = l1.sum()/(m.sum()*x.size(1) + 1e-6)\n",
    "            la = F.mse_loss(zc, zr)\n",
    "            loss = alpha*lc + beta*lr_ + gamma*la\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr += loss.item()\n",
    "        tr /= len(train_loader)\n",
    "        model.eval()\n",
    "        vl, ok, tot = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                m = (torch.rand_like(x[:, :1]) < mask_ratio).float()\n",
    "                xm = x*(1-m)\n",
    "                logits, recon, zc, zr = model(xm)\n",
    "                lc = ce(logits, y)\n",
    "                l1 = torch.abs(recon - x)*m\n",
    "                lr_ = l1.sum()/(m.sum()*x.size(1) + 1e-6)\n",
    "                la = F.mse_loss(zc, zr)\n",
    "                loss = alpha*lc + beta*lr_ + gamma*la\n",
    "                vl += loss.item()\n",
    "                ok += (logits.argmax(1)==y).sum().item()\n",
    "                tot += y.size(0)\n",
    "        vl /= len(val_loader)\n",
    "        acc = ok/tot\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {tr:.4f} | Val Loss: {vl:.4f} | Val Acc: {acc:.4f}\")\n",
    "        if vl < best:\n",
    "            best = vl\n",
    "            bad = 0\n",
    "            best_w = model.state_dict()\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    model.load_state_dict(best_w)\n",
    "    return model\n",
    "\n",
    "def show_reconstructions(model, loader, recon_fn, device, n=6):\n",
    "    model.eval()\n",
    "    imgs, _ = next(iter(loader))\n",
    "    imgs = imgs[:n].to(device)\n",
    "    with torch.no_grad():\n",
    "        recon = recon_fn(model, imgs)\n",
    "    imgs = imgs.cpu().permute(0,2,3,1)\n",
    "    recon = recon.cpu().permute(0,2,3,1)\n",
    "    imgs = (imgs*0.5 + 0.5).clamp(0,1)\n",
    "    recon = (recon*0.5 + 0.5).clamp(0,1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i in range(n):\n",
    "        plt.subplot(2,n,i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2,n,i+1+n)\n",
    "        plt.imshow(recon[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730c8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(nn.Flatten(), nn.Dropout(0.5), nn.Linear(64,2))\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed6a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(0.5), nn.Linear(128,2)\n",
    "        )\n",
    "        self.dec_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,64,4,stride=2,padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,4,stride=2,padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32,3,3,padding=1), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.shared(x)\n",
    "        logits = self.cls_head(z)\n",
    "        recon = self.dec_head(z)\n",
    "        return logits, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b016e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftShareMTL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_cls = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_rec = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU()\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(0.5), nn.Linear(128,2))\n",
    "        self.dec_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64,4,stride=2,padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,4,stride=2,padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32,3,3,padding=1), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        zc = self.encoder_cls(x)\n",
    "        zr = self.encoder_rec(x)\n",
    "        logits = self.cls_head(zc)\n",
    "        recon = self.dec_head(zr)\n",
    "        zc_vec = torch.flatten(F.adaptive_avg_pool2d(zc, 1), 1)\n",
    "        zr_vec = torch.flatten(F.adaptive_avg_pool2d(zr, 1), 1)\n",
    "        return logits, recon, zc_vec, zr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d522090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1200\n",
      "['Forest', 'OOD', 'Residential']\n",
      "Labels présents dans le dataset : [0, 1]\n",
      "Epoch [1/30] | Train Loss: 0.1201 | Val Loss: 0.0055 | Val Acc: 0.9983\n",
      "Epoch [2/30] | Train Loss: 0.0073 | Val Loss: 0.0073 | Val Acc: 0.9975\n",
      "Epoch [3/30] | Train Loss: 0.0082 | Val Loss: 0.0024 | Val Acc: 0.9992\n",
      "Epoch [4/30] | Train Loss: 0.0030 | Val Loss: 0.0020 | Val Acc: 0.9992\n",
      "Epoch [5/30] | Train Loss: 0.0111 | Val Loss: 0.0033 | Val Acc: 0.9983\n",
      "Epoch [6/30] | Train Loss: 0.0055 | Val Loss: 0.0019 | Val Acc: 0.9992\n",
      "Epoch [7/30] | Train Loss: 0.0029 | Val Loss: 0.0018 | Val Acc: 0.9992\n",
      "Epoch [8/30] | Train Loss: 0.0015 | Val Loss: 0.0023 | Val Acc: 0.9992\n",
      "Epoch [9/30] | Train Loss: 0.0048 | Val Loss: 0.0221 | Val Acc: 0.9933\n",
      "Epoch [10/30] | Train Loss: 0.0059 | Val Loss: 0.0012 | Val Acc: 0.9992\n",
      "Epoch [11/30] | Train Loss: 0.0050 | Val Loss: 0.0057 | Val Acc: 0.9983\n",
      "Epoch [12/30] | Train Loss: 0.0054 | Val Loss: 0.0064 | Val Acc: 0.9975\n",
      "Epoch [13/30] | Train Loss: 0.0030 | Val Loss: 0.0017 | Val Acc: 0.9992\n",
      "Epoch [14/30] | Train Loss: 0.0021 | Val Loss: 0.0035 | Val Acc: 0.9983\n",
      "Epoch [15/30] | Train Loss: 0.0004 | Val Loss: 0.0017 | Val Acc: 0.9983\n",
      "Epoch [16/30] | Train Loss: 0.0039 | Val Loss: 0.0041 | Val Acc: 0.9975\n",
      "Epoch [17/30] | Train Loss: 0.0041 | Val Loss: 0.0034 | Val Acc: 0.9983\n",
      "Epoch [18/30] | Train Loss: 0.0069 | Val Loss: 0.0024 | Val Acc: 0.9992\n",
      "Epoch [19/30] | Train Loss: 0.0036 | Val Loss: 0.0067 | Val Acc: 0.9983\n",
      "Epoch [20/30] | Train Loss: 0.0004 | Val Loss: 0.0019 | Val Acc: 0.9992\n",
      "Early stopping triggered.\n",
      "CNN | OOD acc: 0.7333\n",
      "CNN | Forest: 0.4833\n",
      "CNN | Dense: 1.0\n",
      "CNN | Medium: 0.9667\n",
      "CNN | CM OOD:\n",
      " [[29 31]\n",
      " [ 1 59]]\n",
      "Epoch [1/30] | Train Loss: 0.0353 | Val Loss: 0.0217 | Val Acc: 0.9925\n",
      "Epoch [2/30] | Train Loss: 0.0178 | Val Loss: 0.0108 | Val Acc: 0.9983\n",
      "Epoch [3/30] | Train Loss: 0.0140 | Val Loss: 0.4074 | Val Acc: 0.6550\n",
      "Epoch [4/30] | Train Loss: 0.0108 | Val Loss: 0.0664 | Val Acc: 0.9775\n",
      "Epoch [5/30] | Train Loss: 0.0108 | Val Loss: 0.0037 | Val Acc: 1.0000\n",
      "Epoch [6/30] | Train Loss: 0.0084 | Val Loss: 0.0038 | Val Acc: 0.9992\n",
      "Epoch [7/30] | Train Loss: 0.0070 | Val Loss: 0.0048 | Val Acc: 1.0000\n",
      "Epoch [8/30] | Train Loss: 0.0090 | Val Loss: 0.0081 | Val Acc: 0.9983\n",
      "Epoch [9/30] | Train Loss: 0.0068 | Val Loss: 0.0075 | Val Acc: 0.9975\n",
      "Epoch [10/30] | Train Loss: 0.0060 | Val Loss: 0.0039 | Val Acc: 1.0000\n",
      "Epoch [11/30] | Train Loss: 0.0072 | Val Loss: 0.0024 | Val Acc: 1.0000\n",
      "Epoch [12/30] | Train Loss: 0.0055 | Val Loss: 0.0095 | Val Acc: 0.9967\n",
      "Epoch [13/30] | Train Loss: 0.0053 | Val Loss: 0.0037 | Val Acc: 0.9992\n",
      "Epoch [14/30] | Train Loss: 0.0059 | Val Loss: 0.0026 | Val Acc: 1.0000\n",
      "Epoch [15/30] | Train Loss: 0.0094 | Val Loss: 0.0025 | Val Acc: 1.0000\n",
      "Epoch [16/30] | Train Loss: 0.0049 | Val Loss: 0.0026 | Val Acc: 1.0000\n",
      "Epoch [17/30] | Train Loss: 0.0050 | Val Loss: 0.0070 | Val Acc: 0.9983\n",
      "Epoch [18/30] | Train Loss: 0.0038 | Val Loss: 0.0035 | Val Acc: 0.9983\n",
      "Epoch [19/30] | Train Loss: 0.0055 | Val Loss: 0.0021 | Val Acc: 1.0000\n",
      "Epoch [20/30] | Train Loss: 0.0063 | Val Loss: 0.0020 | Val Acc: 1.0000\n",
      "Epoch [21/30] | Train Loss: 0.0080 | Val Loss: 0.0026 | Val Acc: 1.0000\n",
      "Epoch [22/30] | Train Loss: 0.0050 | Val Loss: 0.0025 | Val Acc: 0.9992\n",
      "Epoch [23/30] | Train Loss: 0.0030 | Val Loss: 0.0035 | Val Acc: 0.9983\n",
      "Epoch [24/30] | Train Loss: 0.0040 | Val Loss: 0.0061 | Val Acc: 0.9975\n",
      "Epoch [25/30] | Train Loss: 0.0057 | Val Loss: 0.0032 | Val Acc: 0.9992\n",
      "Epoch [26/30] | Train Loss: 0.0034 | Val Loss: 0.0019 | Val Acc: 1.0000\n",
      "Epoch [27/30] | Train Loss: 0.0041 | Val Loss: 0.0022 | Val Acc: 1.0000\n",
      "Epoch [28/30] | Train Loss: 0.0042 | Val Loss: 0.0035 | Val Acc: 0.9992\n",
      "Epoch [29/30] | Train Loss: 0.0036 | Val Loss: 0.0018 | Val Acc: 1.0000\n",
      "Epoch [30/30] | Train Loss: 0.0027 | Val Loss: 0.0017 | Val Acc: 1.0000\n",
      "MTL | OOD acc: 0.6917\n",
      "MTL | Forest: 0.5833\n",
      "MTL | Dense: 0.9333\n",
      "MTL | Medium: 0.6667\n",
      "MTL | CM OOD:\n",
      " [[35 25]\n",
      " [12 48]]\n",
      "Epoch [1/30] | Train Loss: 0.0681 | Val Loss: 0.0360 | Val Acc: 0.9925\n",
      "Epoch [2/30] | Train Loss: 0.0513 | Val Loss: 0.0336 | Val Acc: 0.9908\n",
      "Epoch [3/30] | Train Loss: 0.0422 | Val Loss: 0.0278 | Val Acc: 0.9917\n",
      "Epoch [4/30] | Train Loss: 0.0396 | Val Loss: 0.0290 | Val Acc: 0.9933\n",
      "Epoch [5/30] | Train Loss: 0.0391 | Val Loss: 0.0271 | Val Acc: 0.9917\n",
      "Epoch [6/30] | Train Loss: 0.0282 | Val Loss: 0.0256 | Val Acc: 0.9908\n",
      "Epoch [7/30] | Train Loss: 0.0363 | Val Loss: 0.0593 | Val Acc: 0.9817\n",
      "Epoch [8/30] | Train Loss: 0.0277 | Val Loss: 0.0266 | Val Acc: 0.9933\n",
      "Epoch [9/30] | Train Loss: 0.0213 | Val Loss: 0.0385 | Val Acc: 0.9900\n",
      "Epoch [10/30] | Train Loss: 0.0228 | Val Loss: 0.0102 | Val Acc: 0.9992\n",
      "Epoch [11/30] | Train Loss: 0.0201 | Val Loss: 0.0305 | Val Acc: 0.9942\n",
      "Epoch [12/30] | Train Loss: 0.0243 | Val Loss: 0.0435 | Val Acc: 0.9867\n",
      "Epoch [13/30] | Train Loss: 0.0219 | Val Loss: 0.0125 | Val Acc: 0.9983\n",
      "Epoch [14/30] | Train Loss: 0.0135 | Val Loss: 0.0097 | Val Acc: 0.9983\n",
      "Epoch [15/30] | Train Loss: 0.0139 | Val Loss: 0.5109 | Val Acc: 0.6925\n",
      "Epoch [16/30] | Train Loss: 0.0140 | Val Loss: 0.0374 | Val Acc: 0.9883\n",
      "Epoch [17/30] | Train Loss: 0.0109 | Val Loss: 0.0068 | Val Acc: 1.0000\n",
      "Epoch [18/30] | Train Loss: 0.0126 | Val Loss: 0.0211 | Val Acc: 0.9958\n",
      "Epoch [19/30] | Train Loss: 0.0114 | Val Loss: 0.0087 | Val Acc: 0.9992\n",
      "Epoch [20/30] | Train Loss: 0.0119 | Val Loss: 0.0063 | Val Acc: 1.0000\n",
      "Epoch [21/30] | Train Loss: 0.0120 | Val Loss: 0.0105 | Val Acc: 0.9983\n",
      "Epoch [22/30] | Train Loss: 0.0125 | Val Loss: 0.0060 | Val Acc: 1.0000\n",
      "Epoch [23/30] | Train Loss: 0.0194 | Val Loss: 0.0062 | Val Acc: 1.0000\n",
      "Epoch [24/30] | Train Loss: 0.0109 | Val Loss: 0.0059 | Val Acc: 1.0000\n",
      "Epoch [25/30] | Train Loss: 0.0089 | Val Loss: 0.0089 | Val Acc: 0.9992\n",
      "Epoch [26/30] | Train Loss: 0.0090 | Val Loss: 0.0722 | Val Acc: 0.9733\n",
      "Epoch [27/30] | Train Loss: 0.0090 | Val Loss: 0.0088 | Val Acc: 0.9983\n",
      "Epoch [28/30] | Train Loss: 0.0086 | Val Loss: 0.0219 | Val Acc: 0.9950\n",
      "Epoch [29/30] | Train Loss: 0.0093 | Val Loss: 0.0108 | Val Acc: 0.9983\n",
      "Epoch [30/30] | Train Loss: 0.0116 | Val Loss: 0.0088 | Val Acc: 1.0000\n",
      "SoftShare | OOD acc: 0.5\n",
      "SoftShare | Forest: 0.0\n",
      "SoftShare | Dense: 1.0\n",
      "SoftShare | Medium: 1.0\n",
      "SoftShare | CM OOD:\n",
      " [[ 0 60]\n",
      " [ 0 60]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    set_seed(1337)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_root = \"../../data\"\n",
    "    ood_root = \"../../data/OOD\"\n",
    "\n",
    "    ds, base, mapping = build_binary_dataset(data_root, keep=(\"Forest\",\"Residential\"), size=(64,64))\n",
    "    train_loader, val_loader = make_loaders(ds, batch_size=32, val_ratio=0.2, seed=1337, num_workers=0)\n",
    "    print(len(train_loader.dataset), len(val_loader.dataset))\n",
    "    print(base.classes)\n",
    "    all_labels = [y for _,y in ds]\n",
    "    unique_labels = torch.unique(torch.tensor(all_labels))\n",
    "    print(\"Labels présents dans le dataset :\", unique_labels.tolist())\n",
    "\n",
    "    model = CNNClassifier().to(device)\n",
    "    model, hist = train_classifier(model, train_loader, val_loader, device, lr=1e-3, epochs=30, patience=10)\n",
    "\n",
    "    ood_base = datasets.ImageFolder(ood_root, transform=make_transform((64,64)))\n",
    "    ood_map = {\n",
    "        ood_base.class_to_idx[\"Forest\"]: 0,\n",
    "        ood_base.class_to_idx[\"DenseResidential\"]: 1,\n",
    "        ood_base.class_to_idx[\"MediumResidential\"]: 1\n",
    "    }\n",
    "    rel_ood = RelabeledDataset(ood_base, ood_map, keep_names=None)\n",
    "    ood_loader = DataLoader(rel_ood, batch_size=64, shuffle=False)\n",
    "    dense_loader = DataLoader(Subset(rel_ood, indices_by_class(ood_base,\"DenseResidential\")), batch_size=64, shuffle=False)\n",
    "    medium_loader = DataLoader(Subset(rel_ood, indices_by_class(ood_base,\"MediumResidential\")), batch_size=64, shuffle=False)\n",
    "    forest_loader = DataLoader(Subset(rel_ood, indices_by_class(ood_base,\"Forest\")), batch_size=64, shuffle=False)\n",
    "\n",
    "    logits_baseline = lambda m,x: m(x)\n",
    "    acc_all, cm_all = evaluate(model, ood_loader, logits_baseline, device)\n",
    "    acc_forest, cm_forest = evaluate(model, forest_loader, logits_baseline, device)\n",
    "    acc_dense, cm_dense = evaluate(model, dense_loader, logits_baseline, device)\n",
    "    acc_medium, cm_medium = evaluate(model, medium_loader, logits_baseline, device)\n",
    "    print(\"CNN | OOD acc:\", round(acc_all,4))\n",
    "    print(\"CNN | Forest:\", round(acc_forest,4))\n",
    "    print(\"CNN | Dense:\", round(acc_dense,4))\n",
    "    print(\"CNN | Medium:\", round(acc_medium,4))\n",
    "    print(\"CNN | CM OOD:\\n\", cm_all.numpy())\n",
    "\n",
    "    model_mtl = MTL_CNN().to(device)\n",
    "    model_mtl, hist_mtl = train_mtl(model_mtl, train_loader, val_loader, device, alpha=0.6, lr=1e-3, epochs=30, patience=10)\n",
    "    logits_mtl = lambda m,x: m(x)[0]\n",
    "    acc_all, cm_all = evaluate(model_mtl, ood_loader, logits_mtl, device)\n",
    "    acc_forest, cm_forest = evaluate(model_mtl, forest_loader, logits_mtl, device)\n",
    "    acc_dense, cm_dense = evaluate(model_mtl, dense_loader, logits_mtl, device)\n",
    "    acc_medium, cm_medium = evaluate(model_mtl, medium_loader, logits_mtl, device)\n",
    "    print(\"MTL | OOD acc:\", round(acc_all,4))\n",
    "    print(\"MTL | Forest:\", round(acc_forest,4))\n",
    "    print(\"MTL | Dense:\", round(acc_dense,4))\n",
    "    print(\"MTL | Medium:\", round(acc_medium,4))\n",
    "    print(\"MTL | CM OOD:\\n\", cm_all.numpy())\n",
    "    recon_mtl = lambda m,x: m(x)[1]\n",
    "\n",
    "    model_soft = SoftShareMTL().to(device)\n",
    "    model_soft = train_soft_share(model_soft, train_loader, val_loader, device, alpha=0.9, beta=0.09, gamma=0.01, mask_ratio=0.65, lr=1e-3, epochs=30, patience=10)\n",
    "    logits_soft = lambda m,x: m(x)[0]\n",
    "    acc_all, cm_all = evaluate(model_soft, ood_loader, logits_soft, device)\n",
    "    acc_forest, cm_forest = evaluate(model_soft, forest_loader, logits_soft, device)\n",
    "    acc_dense, cm_dense = evaluate(model_soft, dense_loader, logits_soft, device)\n",
    "    acc_medium, cm_medium = evaluate(model_soft, medium_loader, logits_soft, device)\n",
    "    print(\"SoftShare | OOD acc:\", round(acc_all,4))\n",
    "    print(\"SoftShare | Forest:\", round(acc_forest,4))\n",
    "    print(\"SoftShare | Dense:\", round(acc_dense,4))\n",
    "    print(\"SoftShare | Medium:\", round(acc_medium,4))\n",
    "    print(\"SoftShare | CM OOD:\\n\", cm_all.numpy())\n",
    "    recon_soft = lambda m,x: m(x)[1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62f810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMLGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
